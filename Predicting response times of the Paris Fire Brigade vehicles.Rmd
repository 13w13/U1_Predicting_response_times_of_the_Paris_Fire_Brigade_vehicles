---
title: "Predicting response times of the Paris Fire Brigade vehicles"
author: "Edgar Jullien, Antoine Settelen, Simon Weiss"
date: '`r Sys.Date()`'

output:
  html_document:
    number_sections: true
    fig_caption: true
    toc: true
---


Liens notebook inspiration artistique : 
Les notebooks qui ont déja fait le challenge : 
https://github.com/quachn/X_PFB
https://github.com/Gguinet/Fire-Brigade-Challenge/blob/master/.ipynb_checkpoints/Code-checkpoint.ipynb




Les notebooks traintant du même type de problème : 
https://medium.com/crim/predicting-the-response-times-of-firefighters-using-data-science-da79f6965f93
https://www.kaggle.com/gaborfodor/from-eda-to-the-top-lb-0-367
https://www.kaggle.com/karelrv/nyct-from-a-to-z-with-xgboost-tutorial
https://www.kaggle.com/headsortails/nyc-taxi-eda-update-the-fast-the-curious


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Introduction

## Presentation of the project

This group project responds to the **prof Nadine Galy instructions** written below :  
The project involves identifying a real-world business problem or opportunity and designing and implementing an analysis plan to address it using at least one of the modelling methods studied in the course. You are free to choose any business problem or opportunity or public policy issue that you consider challenging and useful to address using business analytics.
The data that you use should be readily available and verifiable.

This is a notebook for the [Paris Fire Brigate](https://paris-fire-brigade.github.io/data-challenge/challenge.html) data challenge 2020 with [ENS](https://challengedata.ens.fr/participants/challenges/21/) and [College de France](https://www.college-de-france.fr/site/stephane-mallat/Challenges-2020.htm).


**The goal of this playground challenge** is to predict the *The response times of the Paris Fire Brigade vehicles * which is the delay between:
* the selection of a rescue vehicle (the time when a rescue team is warned) 
* and the rescue team arrival time at the scene of the request (information sent manually via portable radio)

This measurement is composed by the 2 following periods of time: 
* the activation period of the rescue team 
* the transit time of the rescue team

Based on features like trip coordinates, pickup date, type of the arriavl destination, vehicules etc.. 

The [data](https://challengedata.ens.fr/participants/challenges/21/) which covers the entier year 2018 for which inoperable data have been squeezed out comes in the shape of 219 337 training observations and 108 033 test observation. The dataset covers the entire year 
Each row contains one Paris fire brigade intervention.

"Response time is one of the most important factors for firefighters because their ability to save lives and rescue people depends on it. Every fire department in the world seeks strategies to decrease their response time, and several analyses have been conducted in the past years to determine what could impact response time. In the meantime, fire departments have been collecting data on their interventions; yet, few of them actually use data science to develop a data-driven decision making approach."https://medium.com/crim/predicting-the-response-times-of-firefighters-using-data-science-da79f6965f93


"A lot of fire departments and emergency services rely on geographic information systems tools, such as ESRI ARCGis or Network Analyst, to obtain estimations about the response time. These tools rely on computing the shortest route using a graphical representation of the road network, which usually gives an accurate estimate of the travel time. Their drawback is that they cannot always take into consideration external dynamic factors such as the weather, traffic or type of units or intervention. Hence, there is an opportunity for machine learning tools to be used here."



**In this notebook**, we will first study and visualise the original data, engineer new features, and examine potential outliers. Then... 

We hope that this notebook will have good results to the challenge and responds fully to Nadine Galy requirement.
As always, any feedback, questions, or constructive criticism are much appreciated.


## Features description


**Input parameters (x_train.csv and x_test.csv):**

* **[ID]** `emergency vehicle selection`: identifier of the selection instance of an emergency vehicle for an intervention
* Intervention
    * `intervention`: identifier of the intervention
    * Alert reason
        * `alert reason category` (category): alert reason category
        * `alert reason` (category): alert reason
    * Address
        * `intervention on public roads` (boolean): 1 when it concerns an intervention on public roads, 0 otherwise
        * `floor` (int): floor of the intervention
        * `location of the event` (category): qualifies the location of the emergency request, for example: entrance hall, boiler room, motorway, etc.
        * `longitude intervention` (float): approximate longitude of the intervention address. **ATTENTION: `intervention_longitude`** !
        * `latitude intervention` (float): approximate latitude of the intervention address. **ATTENTION: `intervention_latitude`** !
    * Emergency vehicle
        * `emergency vehicle`: identifier of the emergency vehicle
            * `emergency vehicle type` (category): type of the emergency vehicle
            * `rescue center` (category): identifier of the rescue center to which belong the vehicle (parking spot of the emergency vehicle)
        * `selection time` (datetime): selection time of the emergency vehicle
            * `date key selection` (int): selection date in YYYYMMDD format
            * `time key selection` (int): selection time in HHMMSS format
        * State of the emergency vehicle preceding its selection for an intervention
            * Operational status of the vehicle preceding its selection
                * `status preceding selection` (category): status of the emergency vehicle prior to selection. An emergency vehicle is in various statuses during an intervention:
                    * **Selection** - selection of the emergency vehicle by the rescue commitment application
                    * **Departed** - the vehicle starts its route to the location of the emergency request
                    * **Presented** - the vehicle arrives at the location of the request
                    * **Hospital transportation** - the vehicle starts its transport of a victim to hospital
                    * **Hospital arrival** - the vehicle arrives at the hospital
                    * **Leaving hospital** - the vehicle leaves the hospital
                    * **Returned** - the vehicle has returned to its parking spot
                    * **Leave the premises** - because the vehicle can also simply leave the scene of an intervention without having to transport any victim
                    * **Not available** - for various reasons the vehicle can be in an unavailable position
                    * **Not relevant** - statutes without interest
                * `delta status preceding selection-selection` (int): number of seconds before the vehicle was selected when its previous status was entered
            * `departed from its rescue center` (boolean) : 1 when the vehicle departed from its rescue center (emergency vehicle parking spot), 0 otherwise
            * GPS position of the vehicle before departure
                * `longitude before departure` (float): longitude of the position of the vehicle preceding his departure. **ATTENTION: `departure_longitude`** !
                * `latitude previous departure` (float): latitude of the position of the vehicle preceding his departure. **ATTENTION: `departure_latitude`** !
                * `delta position gps previous departure-departure` (int): number of seconds before the selection of the vehicle where its GPS position was recorded (when not parked at its emergency center)
            * GPS tracks
                * `GPS tracks departure-presentation` (float pair list): successive GPS positions (*longitude,latitude;longitude,latitude,* etc.) of the vehicle between departure and presentation. This information is for informational purposes to study vehicle behaviors. (The beacons, emitting the GPS positions of vehicles, are currently not always lit)
                * `GPS tracks departure-presentation datetime` (datetime list): datetime associated with successive GPS positions between the departure and the presentation of the vehicle.
            * Estimated route
                * `OSRM estimated route` (json object): service route response of an OSRM instance (http://project-osrm.org/docs/v5.15.2/api/#route-service) setup with the Ile-de-France OpenStreetMap data
                * `OSRM estimated distance` (float): distance calculated by the OSRM route service
                * `OSRM estimated duration` (float): transit delay calculated by the OSRM route service

**Output parameters (y_train.csv and y_test.csv):**

* **[ID]** `emergency vehicle selection`: identifier of the selection instance of an emergency vehicle for an intervention
* **[TO PREDICT]** `delta selection-departure`(int): elapsed time in seconds between the selection and the departure of the emergency vehicle
* **[TO PREDICT]** `delta departure-presentation`(int): elapsed time in seconds between the departure of the emergency vehicle and its presentation on the intervention scene
* **[TO PREDICT]** `delta selection-presentation `(int): elapsed time in seconds between the selection of the emergency vehicle and its presentation on the intervention scene (delta selection-departure + delta departure-presentation)



**Supplementary files (x_train_additional_file.csv and x_test_additional_file.csv)**

* **[ID]** `emergency vehicle selection`: identifier of the selection instance of an emergency vehicle for an intervention
* `OSRM estimate from last observed GPS position`(json object): service route response from last observed GPS position of an OSRM instance (http://project-osrm.org/docs/v5.15.2/api/#route-service) setup with the Ile-de-France OpenStreetMap data
* `OSRM estimated distance from last observed GPS position`(float): distance (in meters) calculated by the OSRM route service from last observed GPS position
* `OSRM estimated distance from last observed GPS position`(float): distance (in meters) calculated by the OSRM route service from last observed GPS position
* `OSRM estimated duration from last observed GPS position`(float): transit delay (in seconds) calculated by the OSRM route service from last observed GPS position
* `time elapsed between selection and last observed GPS position` (float): in seconds
* `updated OSRM estimated duration` (float): time elapsed (in seconds) between selection and last observed GPS position + OSRM estimated duration from last observed GPS position


Good reading ! 


## 1.1 Load libraries 

```{r}
library(magrittr)
library(data.table)
library(sandwich)
library(dplyr)
library(stringr)
library(ggplot2)
library(DataExplorer)
library(hms)
library(imputeTS)
library(leaflet)
library(Hmisc)
library(FactoMineR)
library(BCA)


```

## 1.2 Load data, transform them into Datatable for better computation time and combine them. 
```{r}
x_train <- read.csv("x_train.csv") %>% setDT
y_train <- read.csv("y_train.csv") %>% setDT
x_test <- read.csv("x_test.csv") %>% setDT
#View(x_train)
data <- cbind(x_train,y_train[,-1])#we don't keep id vehicule selection for no duplicate

#We rename a column which has a special caracter 
c<- colnames(data)
c[14] <- "date.key.selection"
c[15] <- "time.key.selection"
colnames(data) <- c
```


## 1.3 File structure and content
Let's have an overview of the data sets using the *introduce* and *head* tools. First the training data:

```{r}
plot_intro(data)
```

```{r}
glimpse(data)
```

```{r}
plot_intro(x_test)
```

```{r}
glimpse(x_test)
```

We find : 
- we have a great mix of qualitative data and quantitative data
- some quali data are characters such as "status.preceding.selection, other dummy variables coded 0,1 such as $intervention.on.public.roads
- we have NA values
- We have ID variables that we can remove
- We will have to deal with outliers
- Mean of 
** `delta selection-departure`(int): elapsed time in seconds between the selection and the departure of the emergency vehicle
* **[TO PREDICT]** `delta departure-presentation`(int): elapsed time in seconds between the departure of the emergency vehicle and its presentation on the intervention scene
* **[TO PREDICT]** `delta selection-presentation `(int): elapsed time in seconds between the selection of the emergency vehicle and its presentation on the intervention scene (delta selection-departure + delta departure-presentation)


## 1.4 Missing values

```{r}
# visualize missing data
introduce(data)
plot_missing(data)
```

```{r}
introduce(x_train)
plot_missing(x_train)

```


We delete the useless column and the raw with empty cells
```{r}
data <- data[,-24] # delete the column OSMR response json object
data <- data[,-21] #delete the column delta position gps
```

Same with x_test data
```{r}
x_test<-x_test[,-24]
x_test<-x_test[,-21]
```

Let's check how many na it remains
```{r}
sum(is.na(data))
sum(is.na(x_test))
```
Good proporition (around 5% of dataset), acceptable to omit those values. 

```{r}
data <- na.omit(data)

which(is.na(data))
x_test<-na.omit(x_test)
```

(We apply same cleaning in x_train and y_train in case)
```{r}
y_train <- na.omit(y_train)
x_train<-na.omit(x_train)
```
Attention : combine en fonction de 'id intervention


## 1.5 Convertion into right format


Convert qualitative variables into factor
```{r}
data$alert.reason.category<- as.factor(data$alert.reason.category)
data$alert.reason<-as.factor(data$alert.reason)
data$location.of.the.event <- as.factor(data$location.of.the.event)
data$intervention.on.public.roads <- as.factor(data$intervention.on.public.roads)
data$emergency.vehicle.type <- as.factor(data$emergency.vehicle.type)
data$rescue.center <- as.factor(data$rescue.center)
data$status.preceding.selection  <- as.factor(data$status.preceding.selection)
data$departed.from.its.rescue.center  <- as.factor(data$departed.from.its.rescue.center)
data$floor<-as.factor(data$floor)
data$emergency.vehicle<-as.factor(data$emergency.vehicle)
```



Apply same in x_test
```{r}
x_test$alert.reason.category<- as.factor(x_test$alert.reason.category)
x_test$alert.reason<-as.factor(x_test$alert.reason)
x_test$location.of.the.event <- as.factor(x_test$location.of.the.event)
x_test$intervention.on.public.roads <- as.factor(x_test$intervention.on.public.roads )
x_test$emergency.vehicle.type <- as.factor(x_test$emergency.vehicle.type)
x_test$rescue.center <- as.factor(x_test$rescue.center)
x_test$status.preceding.selection  <- as.factor(x_test$status.preceding.selection)
x_test$departed.from.its.rescue.center  <- as.factor(x_test$departed.from.its.rescue.center)
x_test$floor<-as.factor(x_test$floor)
x_test$emergency.vehicle<-as.factor(x_test$emergency.vehicle)

```


Manage *Selection time 

 * `selection time` (datetime): selection time of the emergency vehicle
            * `date key selection` (int): selection date in YYYYMMDD format
            * `time key selection` (int): selection time in HHMMSS format


We can deleat selection time. We keep numeric format for date key selection and time key selection for now (more manageable for cor analysis and regression). 
For EDA part, we could convert them into date and time format


```{r}
data<-data[,-c("selection.time")]
```

Store quali, quali and quali with Ys variables 
```{r}
str(data)
colnames(data)
data.quali<-data[,c(3,4,5,6,7,10,11,12,15,17)]


data.quanti.y<-data[,-c(3,4,5,6,7,10,11,12,15,17)] #with Ys

data.quanti<-data[,-c(3,4,5,6,7,10,11,12,15,17,24,25,26)]
```


# 2. EDA

## 2.1 Intro : map 
We start with a map of Paris and overlay a managable number of coordinates to get a general overview of the locations and distances in question. For this visualisation we use the [leaflet](https://rstudio.github.io/leaflet/) package, which includes a variety of cool tools for interactive maps. In this map you can zoom and pan through the interventation locations:

```{r fig.align = 'default', warning = FALSE, fig.cap ="Fig. 1", out.width="100%"}
set.seed(1234)
foo <- sample_n(data, 8e3)

leaflet(data = foo) %>% addProviderTiles("Esri.NatGeoWorldMap") %>%
  addCircleMarkers(~ longitude.intervention, ~latitude.intervention, radius = 1,
                   color = "blue", fillOpacity = 0.3)

```

Comment maps : 

## 2.2 Dependant Variables Ys
We have 2 dependant variables and 1 global variable

```{r}
summary(y_train[,-1])
```

Extreme values for your Ys variables around 6 hours ! 

Plot density distribution for time of vehicle selection
```{r}

data %>%
  ggplot(aes(delta.selection.departure)) +
  geom_density(fill = "red", bins = 100) +
  scale_x_log10() +
  scale_y_sqrt() + 
  theme_minimal()
```

Note the logarithmic x-axis and square-root y-axis.

We find:

    the majority of vehicule rather timinh follow a rather smooth distribution that looks almost log-normal with a peak just short of 200 seconds, i.e. about 4 minutes.

    There are several suspiciously short rides with less than 10 seconds duration.

    Additionally, there is a strange delta-shaped peak of trip_duration just before the 1e5 seconds mark and even a few way above it:
    
Plot density distribution the time to present to the place
```{r}
data %>%
  ggplot(aes(delta.departure.presentation)) +
  geom_density(fill = "red", bins = 100) +
  scale_x_log10() +
  scale_y_sqrt() + 
  theme_minimal()
```

We find:

    same smooth distribution that looks almost log-normal with a peak around 400, 500 seconds, i.e. about 6 minutes.

From previous observation. 

Global variable : selection to arrival
around 8 to 10 mintues. 



In many fire departments the measurement of turnout time and travel time are done manually. An officer presses a button located in the vehicle to signal his departure and his arrival. This process introduces irregularities and variation in the data, which will need to be cleaned. 



Box plots Ys 

For delta.selection.departure
```{r}
data %>% 
  ggplot(aes(delta.selection.departure)) +
  geom_boxplot() 
```


```{r}
data %>% 
  ggplot(aes(delta.departure.presentation)) +
  geom_boxplot() 
```


Cor between Ys

```{r}
library(Hmisc)
library(corrplot)
corr<-rcorr(as.matrix(y_train[,-c(1,4)])) # We remove global and ID which will be cor
y_train_cor= corr$r

corr
corrplot(y_train_cor, method="square",type="upper", order="hclust", tl.col="black", tl.srt=45)
```

The 2 Ys are not corr between them


### 2.2.1 Manage outliers for Ys values

Plot 
  delta.selection.departure
```{r}
y_train %>% 
  mutate(
      delta.selection.departure_minutes = round(delta.selection.departure / 60, 0),
    duration_grp = case_when(
      between(delta.selection.departure_minutes, 0,  2)  ~ "Less than 2 minutes",
      between(delta.selection.departure_minutes, 2, 4) ~ "2 to 4 minutes",
      between(delta.selection.departure_minutes, 4, 8) ~ "8 to 12 minutes",
      delta.selection.departure_minutes >= 8 ~ "8 or more minutes"
    ),
    duration_grp = factor(duration_grp, 
                          levels = c("Less than 2 minutes", "2 to 4 minutes", "8 to 12 minutes", "12 or more minutes"))
  ) %>%
  group_by(duration_grp) %>% 
  ggplot(aes(x=duration_grp,group=duration_grp)) + 
  geom_bar(fill="#E41A1C") +
  labs(x="Group", y="count") +
  theme_minimal()
```
Most of data is around 0 to 12 max minutes for 


Departure to presentation
```{r}
y_train %>% 
  mutate(
      delta.departure.presentation_minutes = round(delta.departure.presentation / 60, 0),
    duration2_grp = case_when(
      between(delta.departure.presentation_minutes, 0,  5)  ~ "Less than 5 minutes",
      between(delta.departure.presentation_minutes, 5, 10) ~ "5 to 10 minutes",
      between(delta.departure.presentation_minutes, 10, 15) ~ "10 to 15 minutes",
      between(delta.departure.presentation_minutes, 15, 20) ~ "15 to 20 minutes",
      delta.departure.presentation_minutes >= 20 ~ " 20 or more minutes"
    ),
    duration2_grp = factor(duration2_grp, 
                          levels = c("Less than 5 minutes", "5 to 10 minutes", "10 to 15 minutes", "15 to 20 minutes","20 or more minutes" ))
  ) %>%
  group_by(duration2_grp) %>% 
  ggplot(aes(x=duration2_grp,group=duration2_grp)) + 
  geom_bar(fill="#E41A1C") +
  labs(x="Group", y="count") +
  theme_minimal()
```

Max 20 min

```{r}
y_train %>% 
  mutate(
      delta.selection.presentation_minutes = round(delta.selection.presentation / 60, 0),
    duration3_grp = case_when(
      between(delta.selection.presentation_minutes, 0,  5)  ~ "Less than 5 minutes",
      between(delta.selection.presentation_minutes, 5, 10) ~ "5 to 10 minutes",
      between(delta.selection.presentation_minutes, 10, 15) ~ "10 to 15 minutes",
      between(delta.selection.presentation_minutes, 15, 20) ~ "15 to 20 minutes",
      delta.selection.presentation_minutes >= 20 ~ " 20 or more minutes"
    ),
    duration3_grp = factor(duration3_grp, 
                          levels = c("Less than 5 minutes", "5 to 10 minutes", "10 to 15 minutes", "15 to 20 minutes","20 or more minutes" ))
  ) %>%
  group_by(duration3_grp) %>% 
  ggplot(aes(x=duration3_grp,group=duration3_grp)) + 
  geom_bar(fill="#E41A1C") +
  labs(x="Group", y="count") +
  theme_minimal()
```


=> Most of duration are between 0 to 20 min (1200 sec)
We can set our outlier cleaning


```{r}
data<- data[data$delta.selection.departure < 1200,]
data<-data[data$delta.departure.presentation < 1200,]
data<-data[data$delta.selection.presentation < 1200,]
```




### 2.2.2 Relation anaylis between Y and other variables
In this part, we will first try to check if there is relation between our dependant variables and the depedant

Let's study  Y0 'selection-departure', Y1'departure-presentation',Y2'selection-presentation' vs feature vars

data and y without GPS data

```{r}
data.nogps<-data[,-c(21,20)]
```


We sample data in 3000 observations for scatter plot

```{r}
set.seed(1234)
foocor.all <- sample_n(data.nogps, 3000)
```

Let's use here scatter plot. Scatter plot are plotted along two axes, the pattern of the resulting points revealing any correlation present. One pattern of special interest is a linear pattern, where the data has a general look of a line going uphill or downhill. L


For instance, let's plot y0 vs alert.reason.category

```{r}
car::scatterplot(delta.selection.departure ~ OSRM.estimated.duration, data = foocor.all, 
                 smoother = TRUE, grid = TRUE)
```
Here we can see that there no clear linear correlation, maybe a some litlle smooth trend. 
But let's take delta. departure. presentation vs the OSRM estimated duration. 

```{r}
car::scatterplot(delta.departure.presentation~ OSRM.estimated.duration, data = foocor.all, 
                 smoother = TRUE, grid = TRUE)
```
We can observe a clear linear positive correlation. Normal since distance and time of presentation are correlated. 



Let's scattter plot matrix.  We just check the first lines for **ys vs other variables** for now and try to identify visualy if there is some high independant explicative variable. 


Let's start by Y0 'selection-departure' 

Plot 5 first
```{r}
pairs(delta.selection.departure~.,data=foocor.all[,c(1,2,3,4,5,22)],
   main="Simple Scatterplot Matrix",lower.panel = NULL)

```

We find : 

It seems that we dont have clear linear correlation. Let's scatter plot the 5 next features. 

5 next
```{r}
pairs(delta.selection.departure~.,data=foocor.all[,c(6,7,8,9,10,11,22)],
   main="Simple Scatterplot Matrix",lower.panel = NULL)

```

Same. 

5 next

```{r}
pairs(delta.selection.departure~.,data=foocor.all[,c(12,13,14,15,16,17,22)],
   main="Simple Scatterplot Matrix",lower.panel = NULL)

```
Same




5 next
```{r}
pairs(delta.selection.departure~.,data=foocor.all[,c(18,19,20,21,22,23,24)],
   main="Simple Scatterplot Matrix",lower.panel = NULL)

```
Same

We find : 
no clear visual linear relation between yo and other features. We will deeper our correlation analysis in the 4. part. 



Let's quickly check scatter plot matrix for delta.departure.presentaiton. 


```{r}
pairs(delta.departure.presentation~.,data=foocor.all[,c(1,2,3,4,5,23)],
   main="Simple Scatterplot Matrix",lower.panel = NULL)

```
Same

```{r}
pairs(delta.departure.presentation~.,data=foocor.all[,c(6,7,8,9,10,11,23)],
   main="Simple Scatterplot Matrix",lower.panel = NULL)

```

Same

```{r}
pairs(delta.departure.presentation~.,data=foocor.all[,c(12,13,14,15,16,17,23)],
   main="Simple Scatterplot Matrix",lower.panel = NULL)

```
Same

```{r}
pairs(delta.departure.presentation~.,data=foocor.all[,c(18,19,20,21,22,23)],
   main="Simple Scatterplot Matrix",lower.panel = NULL)

```

We find a linear correlation between y1 and OSRM estimated estimated time and distance (which seems to be normal)

We will deeper our correlation analysis in the 4part of EDA. 
But first, let's manage outliers with some EDA with independant variables ! 


## 3. Explanatory data : Manage Outliers

### 3.1 Verify data outliers

#### 3.1.1 Quantitative Variables

##### 3.1.1.1 Histograms
```{r}

which(is.na(data.quanti)) #check for NA values before starting

```

```{r}
hist(data.quanti$longitude.intervention, col="blue",main="Longitude intervention")
```

```{r}
hist(data.quanti$latitude.intervention, col="blue",main="Latitude intervention")
```

```{r}
hist(data.quanti$delta.status.preceding.selection.selection, col="blue",main="delta status preceding selection-selection")
```

```{r}
hist(data.quanti$longitude.before.departure, col="blue",main="longitude before departure ")
```

```{r}
hist(data.quanti$latitude.before.departure, col="blue",main="Latitude before departure ")
```

```{r}
hist(data.quanti$OSRM.estimated.distance, col="blue",main="OSRM estimated distance")
```

```{r}
hist(data.quanti$OSRM.estimated.duration, col="blue",main="OSRM estimated duration")
```

##### 3.1.1.2 Plot Variables Density Distribution 

```{r}
data.quanti %>%
  ggplot(aes(longitude.intervention)) +
  geom_density(fill = "red", bins = 100) +
  scale_x_log10() +
  scale_y_sqrt() + 
  theme_minimal()
```


```{r}
data.quanti %>%
  ggplot(aes(latitude.intervention)) +
  geom_density(fill = "red", bins = 100) +
  scale_x_log10() +
  scale_y_sqrt() + 
  theme_minimal()
```


```{r}
data.quanti %>%
  ggplot(aes(delta.status.preceding.selection.selection)) +
  geom_density(fill = "red", bins = 100) +
  scale_x_log10() +
  scale_y_sqrt() + 
  theme_minimal()
```


```{r}
data.quanti %>%
  ggplot(aes(longitude.before.departure)) +
  geom_density(fill = "red", bins = 100) +
  scale_x_log10() +
  scale_y_sqrt() + 
  theme_minimal()
```


```{r}
data.quanti %>%
  ggplot(aes(latitude.before.departure)) +
  geom_density(fill = "red", bins = 100) +
  scale_x_log10() +
  scale_y_sqrt() + 
  theme_minimal()
```


```{r}
data.quanti %>%
  ggplot(aes(OSRM.estimated.distance)) +
  geom_density(fill = "red", bins = 100) +
  scale_x_log10() +
  scale_y_sqrt() + 
  theme_minimal()
```


```{r}
data.quanti %>%
  ggplot(aes(OSRM.estimated.duration)) +
  geom_density(fill = "red", bins = 100) +
  scale_x_log10() +
  scale_y_sqrt() + 
  theme_minimal()
```

##### 3.1.1.3 Boxplots

```{r}
boxplot(data.quanti$longitude.intervention, col="blue",main="Longitude intervention")
```

```{r}
boxplot(data.quanti$latitude.intervention, col="blue",main="Latitude intervention")
```

```{r}
boxplot(data.quanti$delta.status.preceding.selection.selection , col="blue",main="delta status preceding selection-selection")
```

```{r}
boxplot(data.quanti$longitude.before.departure, col="blue",main="longitude before departure")
```

```{r}
boxplot(data.quanti$latitude.before.departure, col="blue",main="latitude before departure")
```

```{r}
boxplot(data.quanti$OSRM.estimated.distance, col="blue",main="OSRM estimated distance")
```

```{r}
boxplot(data.quanti$OSRM.estimated.duration, col="blue",main="OSRM estimated duration")
```

##### 3.1.1.3 Manage Outliers 

```{r}
data.clean <- data[data$delta.status.preceding.selection.selection < 100000,]
data.clean <- data[data$OSRM.estimated.duration < 1000,]


#Assign the new value to data.quanti
colnames(data.clean)
data.quanti<-data.clean[,-c(3,4,5,6,7,10,11,12,15,17,24,25,26)]
data.quanti.y<-data.clean[,-c(3,4,5,6,7,10,11,12,15,17)]
```

#### 3.1.2 Qualitative Variables

##### 3.1.2.1 Tables 

```{r}
str(data.quali)
```

```{r}
table(data.quali$alert.reason.category)
prop.table(table(data.quali$alert.reason.category))
```

```{r}
table(data.quali$alert.reason) 
prop.table(table(data.quali$alert.reason))
```
Csq -> we need to combine several categories for alert reason variable

```{r}
table(data.quali$intervention.on.public.roads) 
prop.table(table(data.quali$intervention.on.public.roads))
```

```{r}
table(data.quali$floor) 
prop.table(table(data.quali$floor))
```
Csq -> we need to combine several categories for floors

```{r}
table(data.quali$location.of.the.event) 
prop.table(table(data.quali$location.of.the.event))
```
Csq -> we need to combine several categories for location of the event variable


```{r}
table(data.quali$emergency.vehicle)
prop.table(table(data.quali$emergency.vehicle))
```

```{r}
table(data.quali$emergency.vehicle.type)
prop.table(table(data.quali$emergency.vehicle.type))
```
csq -> we need to combine several categories for emergency vehicle type variable

```{r}
table(data.quali$rescue.center)
prop.table(table(data.quali$rescue.center))
```
csq -> we need to combine several categories for rescue center variable

```{r}
table(data.quali$status.preceding.selection)
prop.table(table(data.quali$status.preceding.selection))
```
csq -> we need to combine several categories for status preceding selection variable

```{r}
table(data.quali$departed.from.its.rescue.center)
prop.table(table(data.quali$departed.from.its.rescue.center))
```
csq -> we need to combine several categories for departed from its rescue center variable


##### 3.1.2.2 Barplots 

```{r}
barplot(table(data.quali$alert.reason.category),horiz = F,cex.names=0.8,col="blue",main="Alert Reason Category",ylab="Frequency", plot=TRUE)
```

```{r}
barplot(table(data.quali$alert.reason),horiz = F,cex.names=0.8,col="blue",main="Alert Reason Category",ylab="Frequency", plot=TRUE)
```
Csq -> we need to combine several categories for alert reason variable

```{r}
barplot(table(data.quali$intervention.on.public.roads),horiz = F,cex.names=0.8,col="blue",main="Intervention on public road",ylab="Frequency", plot=TRUE)
```

```{r}
barplot(table(data.quali$floor),horiz = F,cex.names=0.8,col="blue",main="Floors",ylab="Frequency", plot=TRUE)
```
Csq -> we need to combine several categories for floors

```{r}
barplot(table(data.quali$location.of.the.event),horiz = F,cex.names=0.8,col="blue",main="Location of the event",ylab="Frequency", plot=TRUE)
```
Csq -> we need to combine several categories for location of the event variable

```{r}
barplot(table(data.quali$emergency.vehicle),horiz = F,cex.names=0.8,col="blue",main="Emergency Vehicle Type",ylab="Frequency", plot=TRUE)
```


```{r}
barplot(table(data.quali$emergency.vehicle.type),horiz = F,cex.names=0.8,col="blue",main="Emergency Vehicle Type",ylab="Frequency", plot=TRUE)
```
csq -> we need to combine several categories for emergency vehicle type variable

```{r}
barplot(table(data.quali$rescue.center),horiz = F,cex.names=0.8,col="blue",main="Rescue center variable",ylab="Frequency", plot=TRUE)
```
csq -> we need to combine several categories for rescue center variable

```{r}
barplot(table(data.quali$status.preceding.selection),horiz = F,cex.names=0.8,col="blue",main="Emergency Vehicle Type",ylab="Frequency", plot=TRUE)
```

```{r}
barplot(table(data.quali$departed.from.its.rescue.center),horiz = F,cex.names=0.8,col="blue",main="Departed from its rescue center",ylab="Frequency", plot=TRUE)
```
csq -> we need to combine several categories for departed from its rescue center variable


##### 3.1.2.3 Group Together Categories With Low Frequency & Manage Outliers

for the floor variable, group everything below -2 & everything after 17
```{r}

table(data.clean$floor)

levels(data.clean$floor)<-list("-2"=c("-10","-9","-6","-5","-4","-3", "-2"),"-1"=c("-1"),"0"=c("0"),"1"=c("1"),"2"=c("2"),"3"=c("3"),"4"=c("4"),"5"=c("5"),"6"=c("6"),"7"=c("7"),"8"=c("8"),"9"=c("9"),"10"=c("10"),"11"=c("11"),"12"=c("12"),"13"=c("13"),"14"=c("14"),"15"=c("15"),"16"=c("16"),"17"=c("17","18","19","20","21","22","23","24","25","26","27","28","29", "30", "31", "32", "33", "37","52","79","100"))


table(data.clean$floor)

barplot(table(data.clean$floor),horiz = F,cex.names=0.8,col="blue",main="Floors",ylab="Frequency", plot=TRUE)

data.quali<-data.clean[,c(3,4,5,6,7,10,11,12,15,17)]
```

### 3.2. What is time difference between departure.presentation and OSRM estimated duration ? 

Let's compute time diff between departure.presentation(actual) and OSRM estiamted duration (predicted). 
This diff can't be taken into acount in our futur model. 
```{r}
time.diff<-data.clean$delta.departure.presentation-data.clean$OSRM.estimated.duration
time.diff %>% head
hist(time.diff, col="blue",main="Time diff")
summary(time.diff)
```
Here we can see that thre is a median 90 s (1min 30) of time diffirence between OSRM estimated duration and actual timing. 
One max at 1000 second. 
There is also min value which means that the brigate can also drive quickier than what OSRM predicted. 



Normality study [if time]
transform variables
data.clean$log.longitude.intervention <- log(data.clean$longitude.intervention)
data.clean$log.latitude.intervention<- log(data.clean$latitude.intervention)
data.clean$log.delta.status.preceding.selection.selection <- log(data.clean$delta.status.preceding.selection.selection)
data.clean$log.delta.position.gps.previous.departure.departure <- log(data.clean$delta.position.gps.previous.departure.departure) 
data.clean$log.longitude.before.departure <- log(data.clean$longitude.before.departure)
data.clean$log.latitude.before.departure <- log(data.clean$latitude.before.departure)
data.clean$log.OSRM.estimated.distance <- log(data.clean$OSRM.estimated.distance)
data.clean$log.OSRM.estimated.duration   <- log(data.clean$OSRM.estimated.duration)


Histograms
hist(data.clean$log.longitude.intervention, col="blue",main="Log longitude intervention")
hist(data.clean$log.latitude.intervention, col="blue",main=" Log Latitutde intervention")
hist(data.clean$log.delta.status.preceding.selection.selection , col="blue",main=" Log delta status preceding selection-selection")
hist(data.clean$log.delta.position.gps.previous.departure.departure, col="blue",main="Log delta position gps previous departure-departure (int)")
hist(data.clean$log.longitude.before.departure, col="blue",main="Log longitude before departure ")
hist(data.clean$log.latitude.before.departure, col="blue",main="Log Latitude before departure ")
hist(data.clean$log.OSRM.estimated.distance, col="blue",main="Log OSRM estimated distance")
hist(data.clean$log.OSRM.estimated.duration, col="blue",main="Log OSRM estimated duration")


Boxplot
boxplot(data.clean$log.longitude.intervention, col="blue",main="Log longitude intervention")
boxplot(data.clean$log.latitude.intervention, col="blue",main=" Log Latitutde intervention")
boxplot(data.clean$log.delta.status.preceding.selection.selection , col="blue",main=" Log delta status preceding selection-selection")
boxplot(data.clean$log.delta.position.gps.previous.departure.departure, col="blue",main="Log delta position gps previous departure-departure (int)")
boxplot(data.clean$log.longitude.before.departure, col="blue",main="Log longitude before departure ")
boxplot(data.clean$log.latitude.before.departure, col="blue",main="Log Latitude before departure ")
boxplot(data.clean$log.OSRM.estimated.distance, col="blue",main="Log OSRM estimated distance")
boxplot(data.clean$log.OSRM.estimated.duration, col="blue",main="Log OSRM estimated duration")
summary(data.clean)


Normality study
QQ Plot
when the variable is normally distributed
points are aligned on the first bisector

qqnorm(data.clean$floor, main="Normality floor")
qqline(data.clean$floor)

qqnorm(data.clean$longitude.intervention, main="Normality longitude intervention")
qqline(data.clean$longitude.intervention)
qqnorm(data.clean$log.longitude.intervention, main="Normality log longitude intervention")
qqline(data.clean$log.longitude.intervention)

qqnorm(data.clean$latitude.intervention, main="Normality latitude intervention")
qqline(data.clean$latitude.intervention)
qqnorm(data.clean$log.latitude.intervention, main="Normality log latitude intervention")
qqline(data.clean$log.latitude.intervention)

qqnorm(data.clean$delta.status.preceding.selection.selection, main="Normality delta status preceding selection-selection")
qqline(data.clean$delta.status.preceding.selection.selection)
qqnorm(data.clean$log.delta.status.preceding.selection.selection, main="Normality log delta status preceding selection-selectio")
not possible

qqnorm(data.clean$delta.position.gps.previous.departure.departure, main="Normality delta status preceding selection-selection")
qqline(data.clean$delta.position.gps.previous.departure.departure)
qqnorm(data.clean$log.delta.position.gps.previous.departure.departure, main="Normality log delta status preceding selection-selection")
not possible

qqnorm(data.clean$longitude.before.departure, main="Normality longitude before departure")
qqline(data.clean$longitude.before.departure)
qqnorm(data.clean$log.longitude.before.departure, main="Normality log longitude before departure")
qqline(data.clean$log.longitude.before.departure)

qqnorm(data.clean$latitude.before.departure, main="Normality latitude before departure")
qqline(data.clean$latitude.before.departure)
qqnorm(data.clean$log.latitude.before.departure, main="Normality log latitude before departure")
qqline(data.clean$log.latitude.before.departure)

qqnorm(data.clean$OSRM.estimated.distance, main="NormalityOSRM estimated distance")
qqline(data.clean$OSRM.estimated.distance)
qqnorm(data.clean$log.OSRM.estimated.distance, main="Normality log OSRM estimated distance")
qqline(data.clean$log.OSRM.estimated.distance)

qqnorm(data.clean$OSRM.estimated.duration, main="Normality OSRM estimated duration")
qqline(data.clean$OSRM.estimated.duration)
qqnorm(data.clean$log.OSRM.estimated.duration, main="Normality log OSRM estimated duration")
qqline(data.clean$log.OSRM.estimated.duration)


## 4. Feature engineering

In this section we build new features from the existing ones, trying to find better predictors for our target variable. We prefer to define all these new features in a saingle code block below and then study them in the following subsections. 


### 4.1: Speed [km/h]

esimated speed by OSRM
distance / time

```{r}
data.fe<-data.clean
data.fe$OSRM.estimated.speed<-(data.clean$OSRM.estimated.distance/1000)/(data.clean$OSRM.estimated.duration/60^2)
data.fe$OSRM.estimated.speed %>% summary()
hist(data.fe$OSRM.estimated.speed , col="blue",main="Estimated Speed km/h")
```

#### 4.1.2 How top influencer predictors and estimated speed correlate ? 

```{r}
data.fe.sample <- sample_n(data.fe, 100)
```


```{r}
data.fe.sample %>%
  ggplot(aes(x=alert.reason, y=OSRM.estimated.speed,group=alert.reason)) +
  geom_boxplot()
```


```{r}
data.fe.sample %>%
  ggplot(aes(x=rescue.center, y=OSRM.estimated.speed)) +
  geom_boxplot()
```


```{r}
data.fe.sample %>%
  ggplot(aes(x=alert.reason.category, y=OSRM.estimated.speed)) +
  geom_boxplot()
```


Interpretation a rajouter


### 4.2 Add month, day of week, morning, day,  night

```{r}
data.fe$month <-month(as.Date(as.character(data.fe$date.key.selection), "%Y%m%d"))

data.fe$weekdays <-weekdays(as.Date(as.character(data.fe$date.key.selection), "%Y%m%d"))
```




```{r}
data.fe$hours <-as.hms(formatC(as.integer(data.fe$time.key.selection), big.mark = ":", big.interval = 2L))

data.fe$hours <- replace(data.fe$hours, is.na(data.fe$hours), "00:00:00")
```

```{r}
data.fe$hours <- substr(data.fe$hours, 1, 2)

```


```{r}
table(data.fe$month)
prop.table(table(data.fe$month))
```

```{r}
barplot(table(data.fe$month),horiz = F,cex.names=0.8,col="blue",main="Month",ylab="Frequency", plot=TRUE)
```

```{r}
table(data.fe$weekdays)
prop.table(table(data.fe$weekdays))
```

```{r}
barplot(table(data.fe$weekdays),horiz = F,cex.names=0.8,col="blue",main="Day",ylab="Frequency", plot=TRUE)
```


```{r}
table(data.fe$hours)
prop.table(table(data.fe$hours))
```

```{r}
barplot(table(data.fe$hours),horiz = F,cex.names=0.8,col="blue",main="Hours",ylab="Frequency", plot=TRUE)
```


We can now remove date.key.selection and time.key.selection

```{r}
data.fe<-data.fe[,-c(13,14)]

```
Convert time and date build variables into factors

```{r}
data.fe$hours <- as.factor(data.fe$hours)
data.fe$weekdays <- as.factor(data.fe$weekdays)
data.fe$month <- as.factor(data.fe$month)

```


### 4.3 Order of the brigade


For the same event, multiple brigade can leave. 
It is worth to take into account that it might be a corr between the order of the brigate to leave and the time for preparation. 
Once we have classify those brigate, we will be able to remove the id of the intervention. 


```{r}
data.fe$intervention %>% as.factor() %>% str
data.fe$emergency.vehicle.selection %>% str

204987-196480
```
Plot frequency order of id intervention.


```{r}
a<-data.fe %>%
        group_by(intervention)%>%
         tally()
```

```{r}
a %>% setDT
a %>% str
```


```{r}
data.fe<-merge(a,data.fe, by.x = 'intervention', by.y = 'intervention', all=FALSE)
```
rename and convert as factor

```{r}
data.fe$n<-data.fe$n %>% as.factor
```
Now that we have intervention frequency as factor, we can remove intervention ID

```{r}
data.fe<-data.fe[,-c(1)]
```



### 4.4 GPS
We decide to remove GPS data since we have the same data. 
```{r}
data.fe<-data.fe[,-c(16,17)]
```

## 5. Correlation analysis
update data.quanti and data.quali
```{r}
colnames(data.fe)
str(data.fe)

data.quanti<-data.fe[,c(21,17,16,14,9,8,2)]

data.quanti.y<-data.fe[,c(21,20,19,18,17,16,14,9,8,2)]

data.quali<-data.fe[,-c(21,20,19,18,17,16,14,9,8,2)]

data.quali.y<-data.fe[,-c(21,17,16,14,9,8,2)]

```


After engineering new features and before starting the modelling, we have to visualise the relations between our parameters using a correlation matrix. For this, we need to change all the factor features into a numerical format. The visualisation uses the corrplot function from the eponymous package. Corrplot gives us great flexibility in manipulating the style of our plot.

What we see below, are the colour-coded correlation coefficients for each combination of two features. In simplest terms: this shows whether two features are connected so that one changes with a predictable trend if you change the other. The closer this coefficient is to zero the weaker is the correlation. Both 1 and -1 are the ideal cases of perfect correlation and anti-correlation (dark blue and dark red in the plots below).

Here, we are of course interested if and how strongly our correlate with the ys. But we also want to know whether our potential predictors are correlated among each other, so that we can reduce the collinearity in our data set and improve the robustness of our prediction.


```{r}
data.fe %>%
  select(-emergency.vehicle.selection) %>%
  mutate(n = as.integer(n),
         alert.reason = as.integer(alert.reason),
         floor = as.integer(floor),
         emergency.vehicle = as.integer(emergency.vehicle),
         rescue.center = as.integer(rescue.center),
         delta.selection.presentation = as.integer(delta.selection.presentation),
         month = as.integer(month),
         hours = as.integer(hours),
        weekdays = as.integer(weekdays),
         alert.reason.category = as.integer(alert.reason.category),
         intervention.on.public.roads = as.integer(intervention.on.public.roads),
         location.of.the.event = as.integer(location.of.the.event),
         emergency.vehicle.type = as.integer(emergency.vehicle.type),
         status.preceding.selection = as.integer(status.preceding.selection),
          departed.from.its.rescue.center = as.integer(departed.from.its.rescue.center))%>%

  cor(use="complete.obs", method = "spearman") %>%
  corrplot(type="lower", method="pie",order="hclust", 
         diag=FALSE)

```
Add coef

```{r}
data.fe %>%
  select(-emergency.vehicle.selection) %>%
  mutate(n = as.integer(n),
         alert.reason = as.integer(alert.reason),
         floor = as.integer(floor),
         emergency.vehicle = as.integer(emergency.vehicle),
         rescue.center = as.integer(rescue.center),
         delta.selection.presentation = as.integer(delta.selection.presentation),
         month = as.integer(month),
         hours = as.integer(hours),
        weekdays = as.integer(weekdays),
         alert.reason.category = as.integer(alert.reason.category),
         intervention.on.public.roads = as.integer(intervention.on.public.roads),
         location.of.the.event = as.integer(location.of.the.event),
         emergency.vehicle.type = as.integer(emergency.vehicle.type),
         status.preceding.selection = as.integer(status.preceding.selection),
          departed.from.its.rescue.center = as.integer(departed.from.its.rescue.center))%>%

  cor(use="complete.obs", method = "spearman") %>%
  corrplot(type="lower", method="square",order="hclust", 
         addCoef.col = "black", diag=FALSE)

```

We find : 

- alert reason is correlated to alert reason.category (0,65). We can remove alert.reason
- OSRM estimated speed and OSRM distance are correlated (this is quite normal). We decide to keep the 2 features. 
- Statut departure before selection and departed from its rescue center are perfectly correlated. We remove statut departure before selection. 
- Except for ORSM features, there is no high correlated feature. 

```{r}
data.fe<-data.fe[,-c("alert.reason","status.preceding.selection")]
```

##### PCA

A voir si on garde pca

For the PCA, we analayse our variables 
We center the data


```{r}
res.pca=PCA(data.fe, scale.unit=TRUE, ncp=11, graph=F,quali.sup =-c(21,20,19,18,17,16,14,9,8,2))
```

Some error with quali variable. We will convert them into quanti

```{r}
data.variables.PCA <- data.fe %>%
  select(-emergency.vehicle.selection) %>%
  mutate(n = as.integer(n),
         floor = as.integer(floor),
         emergency.vehicle = as.integer(emergency.vehicle),
         rescue.center = as.integer(rescue.center),
         delta.selection.presentation = as.integer(delta.selection.presentation),
         month = as.integer(month),
         hours = as.integer(hours),
        weekdays = as.integer(weekdays),
         alert.reason.category = as.integer(alert.reason.category),
         intervention.on.public.roads = as.integer(intervention.on.public.roads),
         location.of.the.event = as.integer(location.of.the.event),
         emergency.vehicle.type = as.integer(emergency.vehicle.type),
          departed.from.its.rescue.center = as.integer(departed.from.its.rescue.center)) %>% setDT
```


```{r}
data.variables.PCA <- scale(data.variables.PCA, center = TRUE)
```


```{r}

```


The "variance" row gives the amount of variance (or "eigenvalue") in the original variables accounted for by each component.When a component has an eigenvalue greater than 1, it means that it contains more information (ie. explains more variance in the dataset) than one individual variable. 

```{r}
barplot(res.pca$eig[,1], main="Eigenvalues", names.arg=1:nrow(res.pca$eig))
```


We can stop at Dimension 19 because we have reached a cumulative variance of 99%


```{r}
dimdesc(res.pca)
```



```{r}
round(res.pca$eig,7)
```
The first 2 axes account for 40% of the total inertia. 
```{r}
inertia=res.pca$eig[,2]
barplot(inertia,ylab="% Unertia",names.arg=round(inertia,7))+
title("eigenvalues (inertias or variances of each component) in %")
```

```{r}
res.pca=PCA(data.variables.PCA, scale.unit=TRUE, ncp=7, graph=F)
graph.var (res.pca, new.plot=TRUE)
```

```{r}
graph.var (res.pca, lim.cos2.var=0.5, new.plot=TRUE)
```



Qualitatives variables 
```{r}
tapply(data.clean$delta.departure.presentation, data.clean$alert.reason.category, summary)
bartlett.test(data.clean$delta.departure.presentation, data.clean$alert.reason.category)
bartlett.test(data.clean$delta.departure.presentation, data.clean$intervention.on.public.roads) #pvalue <0,05
bartlett.test(data.clean$delta.departure.presentation, data.clean$status.preceding.selection) #pvalue <0,05
bartlett.test(data.clean$delta.departure.presentation, data.clean$departed.from.its.rescue.center)#pvalue <0,05
bartlett.test(data.clean$delta.departure.presentation, data.clean$month)#pvalue <0,05

data.clean$departed.from.its.rescue.center
```

```{r}
data.variables.without.cor <- na.omit(data.variables.without.cor)
```


## OLS - Regression

### Split data

```{r}
dataset$Sample <- create.samples(data.fe, est = 0.70, val = 0.30, rand.seed = 1)
trainingset<-dataset[dataset$Sample=="Estimation",]
testset<-dataset[dataset$Sample=="Validation",]
trainingset<-trainingset[,-32]
testset<-testset[,-32]


```


Split data: train et test
On predict sur le test du train. 
Faire comparaison de modele 


On garde x_test pour le final predict

```{r}
#Linear Regression,
str(data.variables.without.cor)

options(max.print = 10000)

lm <- lm(delta.selection.presentation ~ 
           alert.reason.category
         + intervention.on.public.roads
         + floor
         + location.of.the.event
         + longitude.intervention
         + latitude.intervention
         + emergency.vehicle.type
         + rescue.center
         + status.preceding.selection
         + delta.status.preceding.selection.selection
         + departed.from.its.rescue.center
         + OSRM.estimated.distance
         + OSRM.estimated.duration
           ,data = data.variables.without.cor)

summary(lm)


```
# We can remove the rescue center because he has the higher pvalue
```{r}
lm1 <- lm(delta.selection.presentation ~ 
           alert.reason.category
         + intervention.on.public.roads
         + floor
         + location.of.the.event
         + longitude.intervention
         + latitude.intervention
         + emergency.vehicle.type
         + status.preceding.selection
         + delta.status.preceding.selection.selection
         + departed.from.its.rescue.center
         + OSRM.estimated.distance
         + OSRM.estimated.duration
           ,data = data.variables.without.cor)

summary(lm1)
```

# We  remove the departed.from.its.rescue.center because he has the higher pvalue
```{r}
lm2 <- lm(delta.selection.presentation ~ 
           alert.reason.category
         + intervention.on.public.roads
         + floor
         + location.of.the.event
         + longitude.intervention
         + latitude.intervention
         + emergency.vehicle.type
         + status.preceding.selection
         + delta.status.preceding.selection.selection
         + OSRM.estimated.distance
         + OSRM.estimated.duration
           ,data = data.variables.without.cor)

summary(lm2)
```

# Multicolinéarity 

```{r}
library(car)

vif(lm2)

#cor(data.quanti.without.cor)

lm3 <- lm(delta.selection.presentation ~ 
           alert.reason.category
         + intervention.on.public.roads
         + floor
         + location.of.the.event
         + longitude.intervention
         + latitude.intervention
         + emergency.vehicle.type
         + status.preceding.selection
         + delta.status.preceding.selection.selection
         + OSRM.estimated.distance
           ,data = data.variables.without.cor)

summary(lm3)

vif(lm3)
#variables qualitatives 

```
# Residuals analysis

Study the residuals of the selected model
What we've done is not enough to validate the model. We need to study the residuals
if hypothesis are not validated (see course.), the test of the coefficient are false 
Study the residuals of the selected model

## Is residuals means 0?

```{r}
#mean of residuals
summary(lm3$residuals) 
```
Yes it is !

## Are residuals normally distributed?

Normality test : shapiro test
H0 : Normality and H1 : no normality
```{r}

shapiro.test(lm3$residuals)

#Error in shapiro.test(regmult3$residuals) : la taille de l'échantillon doit être comprise entre 3 et 5000

library(tseries)

jarque.bera.test(lm3$residuals)

#install.packages("nortest")
library(nortest)
ad.test(lm3$residuals)


```



No normality because p-value is << 5%. Here, residuals are not normally distributed. 
NB : non normality could appear because of outliers. This is not the case here. 



```{r}

qqnorm(lm3$residuals)
qqline(lm3$residuals)
```

## Are residuals homoskedastic?

1. First model which is not considering heteroskedasticity

```{r}

plot(lm3$residuals~lm3$fitted)

library(lmtest)

# Breush Pagan test H0 : homoskedasticity against H1 : heteroskedasticity 

bptest(lm3)

```
pvalue << 5%, we reject H0
residuals are heteroskedastic

In case of heteroskedasticity, we have to use a robust standard error estimator. Otherwise, all our t-tests will be wrong.


2. Second model taking into account heteroskedasticity

```{r}

library(sandwich)

#Calculate the robust covariance matrix

vcov2 <- vcovHC(lm3, type = "HC1")

coeftest(lm3, vcov. = vcov2)

#The estimated value of the coefficients remains
#the same but the value of the t-test changes.
#A coefficient could be significant in the first
#model and no more in the second
```

# Are the residuals correlated ?

There are several tests for autocorrelation
Durbin-Watson test is one of the most often used

H0 : Residuals are non autocorrelated
H1 : Residuals are autocorrelated

1. First model which is not considering heteroskedasticity

```{r}

library(car)

# Durbin Watson test where H0 : residuals are not autocorrelated against H1 : residuals are aucorrelated
durbinWatsonTest (lm3,max.lag=1)

# doesn't work....
```

2. Second model taking into account heteroskedasticity

```{r}

library(sandwich)

#Calculate the robust covariance matrix

vcov4 <- NeweyWest(lm3)

coeftest(lm3, vcov. = vcov2)

#The estimated value of the coefficients remains
#the same but the value of the t-test changes.
#A coefficient could be significant in the first
#model and no more in the second
```


Just for test 
```{r}
lm.selection.departure <- lm(data.clean$delta.selection.departure~., data=data.clean[,-12])

summary(lm.selection.departure)

lm1.selection.departure<-(data.clean$delta.selection.departure ~data.clean$delta.status.preceding.selection.selection+factor(data.clean$intervention.on.public.roads), data=data.clean)
summary(lm1.selection.departure)

```


```{r}
library(tree)
tree1.train <-  tree(delta.selection.departure~.,data=data.clean[,-c(7,1,11,12)])
summary(tree1.train)
plot(tree1.train)
text(tree1.train,pretty = 0)

tree1.predict<-predict(tree1.train, newdata=x_test)

```

